{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "color-strength",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--model MODEL] --mot_dir MOT_DIR\n",
      "                             [--detection_dir DETECTION_DIR]\n",
      "                             [--output_dir OUTPUT_DIR]\n",
      "ipykernel_launcher.py: error: the following arguments are required: --mot_dir\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ga27qef/thesis/code/deep_sort/venv/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3426: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import errno\n",
    "import argparse\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as F\n",
    "from torchvision import models\n",
    "\n",
    "\n",
    "def _run_in_batches(f, data_dict, out, batch_size):\n",
    "    data_len = len(out)\n",
    "    num_batches = int(data_len / batch_size)\n",
    "\n",
    "    s, e = 0, 0\n",
    "    for i in range(num_batches):\n",
    "        s, e = i * batch_size, (i + 1) * batch_size\n",
    "        batch_data_dict = {k: v[s:e] for k, v in data_dict.items()}\n",
    "        out[s:e] = f(batch_data_dict)\n",
    "    if e < len(out):\n",
    "        batch_data_dict = {k: v[e:] for k, v in data_dict.items()}\n",
    "        out[e:] = f(batch_data_dict)\n",
    "\n",
    "\n",
    "def extract_image_patch(image, bbox, patch_shape):\n",
    "    bbox = np.array(bbox)\n",
    "    if patch_shape is not None:\n",
    "        ##correct aspect ratio to patch shape\n",
    "        target_aspect = float(patch_shape[1]) / patch_shape[0]\n",
    "        new_width = target_aspect * bbox[3]\n",
    "        bbox[0] -= (new_width - bbox[2]) / 2\n",
    "        bbox[2] = new_width\n",
    "\n",
    "    ##convert to top left, bottom right\n",
    "    bbox[2:] += bbox[:2]\n",
    "    bbox = bbox.astype(np.int)\n",
    "\n",
    "    ##clip at image boundaries\n",
    "    bbox[:2] = np.maximum(0, bbox[:2])\n",
    "    bbox[2:] = np.minimum(np.asarray(image.shape[:2][::-1]) - 1, bbox[2:])\n",
    "    if np.any(bbox[:2] >= bbox[2:]):\n",
    "        return None\n",
    "    sx, sy, ex, ey = bbox\n",
    "    image = image[sy:ey, sx:ex]\n",
    "    image = cv2.resize(image, tuple(patch_shape[::-1]))\n",
    "    return image\n",
    "\n",
    "\n",
    "def normalize_image(image):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    image_normalized = transform(image)\n",
    "    image_normalized = image_normalized.unsqueeze(0)\n",
    "    return image_normalized\n",
    "\n",
    "\n",
    "def ResNet50(image_normalized):\n",
    "    cnn = torchvision.models.resnet50(pretrained=True)\n",
    "    cnn = torch.nn.Sequential(*(list(cnn.children())[:-1])) \n",
    "    features = cnn(image_normalized)\n",
    "    result.view(2048)\n",
    "    return features\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    ##command line arguments\n",
    "    parser = argparse.ArgumentParser(description=\"Re-ID feature extractor\")\n",
    "    parser.add_argument(\n",
    "        \"--model\",\n",
    "        default=\"resources/networks/mars-small128.pb\",\n",
    "        help=\"Path to freezed inference graph protobuf.\")\n",
    "    parser.add_argument(\n",
    "        \"--mot_dir\", help=\"Path to MOTChallenge directory (train or test)\",\n",
    "        required=True)\n",
    "    parser.add_argument(\n",
    "        \"--output_dir\", help=\"Output directory. Will be created if it does not\"\n",
    "        \" exist.\", default=\"detections\")\n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "def main():\n",
    "    args = parse_args()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
