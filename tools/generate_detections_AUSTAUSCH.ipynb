{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "color-strength",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vim: expandtab:ts=4:sw=4\n",
    "import os\n",
    "import errno\n",
    "import argparse\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as F\n",
    "from torchvision import models\n",
    "\n",
    "def _run_in_batches(f, data_dict, out, batch_size):\n",
    "    data_len = len(out)\n",
    "    num_batches = int(data_len / batch_size)\n",
    "\n",
    "    s, e = 0, 0\n",
    "    for i in range(num_batches):\n",
    "        s, e = i * batch_size, (i + 1) * batch_size\n",
    "        batch_data_dict = {k: v[s:e] for k, v in data_dict.items()}\n",
    "        out[s:e] = f(batch_data_dict)\n",
    "    if e < len(out):\n",
    "        batch_data_dict = {k: v[e:] for k, v in data_dict.items()}\n",
    "        out[e:] = f(batch_data_dict)\n",
    "\n",
    "\n",
    "def extract_image_patch(image, bbox, patch_shape):\n",
    "    bbox = np.array(bbox)\n",
    "    if patch_shape is not None:\n",
    "        # correct aspect ratio to patch shape\n",
    "        target_aspect = float(patch_shape[1]) / patch_shape[0]\n",
    "        new_width = target_aspect * bbox[3]\n",
    "        bbox[0] -= (new_width - bbox[2]) / 2\n",
    "        bbox[2] = new_width\n",
    "\n",
    "    # convert to top left, bottom right\n",
    "    bbox[2:] += bbox[:2]\n",
    "    bbox = bbox.astype(np.int)\n",
    "\n",
    "    # clip at image boundaries\n",
    "    bbox[:2] = np.maximum(0, bbox[:2])\n",
    "    bbox[2:] = np.minimum(np.asarray(image.shape[:2][::-1]) - 1, bbox[2:])\n",
    "    if np.any(bbox[:2] >= bbox[2:]):\n",
    "        return None\n",
    "    sx, sy, ex, ey = bbox\n",
    "    image = image[sy:ey, sx:ex]\n",
    "    image = cv2.resize(image, tuple(patch_shape[::-1]))\n",
    "    return image\n",
    "\n",
    "\n",
    "def normalize_image(image)\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    image_normalized = transform(image)\n",
    "    image_normalized = image_normalized.unsqueeze(0)\n",
    "    return image_normalized\n",
    "\n",
    "\n",
    "def ResNet(image_normalized)\n",
    "    cnn = torchvision.models.resnet50(pretrained=True)\n",
    "    cnn = torch.nn.Sequential(*(list(cnn.children())[:-1])) \n",
    "    features = cnn(image_normalized)\n",
    "    result.view(2048)\n",
    "    return features\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    \"\"\"Parse command line arguments.\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser(description=\"Re-ID feature extractor\")\n",
    "    parser.add_argument(\n",
    "        \"--model\",\n",
    "        default=\"resources/networks/mars-small128.pb\",\n",
    "        help=\"Path to freezed inference graph protobuf.\")\n",
    "    parser.add_argument(\n",
    "        \"--mot_dir\", help=\"Path to MOTChallenge directory (train or test)\",\n",
    "        required=True)\n",
    "    parser.add_argument(\n",
    "        \"--detection_dir\", help=\"Path to custom detections. Defaults to \"\n",
    "        \"standard MOT detections Directory structure should be the default \"\n",
    "        \"MOTChallenge structure: [sequence]/det/det.txt\", default=None)\n",
    "    parser.add_argument(\n",
    "        \"--output_dir\", help=\"Output directory. Will be created if it does not\"\n",
    "        \" exist.\", default=\"detections\")\n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "def main():\n",
    "    args = parse_args()\n",
    "    encoder = create_box_encoder(args.model, batch_size=32)\n",
    "    generate_detections(encoder, args.mot_dir, args.output_dir,\n",
    "                        args.detection_dir)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
