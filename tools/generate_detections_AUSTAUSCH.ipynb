{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "color-strength",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import errno\n",
    "import argparse\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as F\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "forty-championship",
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "\n",
    "def _run_in_batches(f, data_dict, out, batch_size):\n",
    "    data_len = len(out)\n",
    "    num_batches = int(data_len / batch_size)\n",
    "\n",
    "    s, e = 0, 0\n",
    "    for i in range(num_batches):\n",
    "        s, e = i * batch_size, (i + 1) * batch_size\n",
    "        batch_data_dict = {k: v[s:e] for k, v in data_dict.items()}\n",
    "        out[s:e] = f(batch_data_dict)\n",
    "    if e < len(out):\n",
    "        batch_data_dict = {k: v[e:] for k, v in data_dict.items()}\n",
    "        out[e:] = f(batch_data_dict)\n",
    "        \n",
    "        \n",
    "\"\"\"     def __call__(self, data_x, batch_size=32):\n",
    "            out = np.zeros((len(data_x), self.feature_dim), np.float32)\n",
    "            _run_in_batches(\n",
    "            lambda x: self.session.run(self.output_var, feed_dict=x),\n",
    "            {self.input_var: data_x}, out, batch_size)\n",
    "            return out\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fitted-valentine",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Extrahiert Bounding Boxen\n",
    "\n",
    "def extract_image_patch(image, bbox, patch_shape):\n",
    "    bbox = np.array(bbox)\n",
    "    if patch_shape is not None:\n",
    "        ##correct aspect ratio to patch shape\n",
    "        target_aspect = float(patch_shape[1]) / patch_shape[0]\n",
    "        new_width = target_aspect * bbox[3]\n",
    "        bbox[0] -= (new_width - bbox[2]) / 2\n",
    "        bbox[2] = new_width\n",
    "\n",
    "    ##convert to top left, bottom right\n",
    "    bbox[2:] += bbox[:2]\n",
    "    bbox = bbox.astype(np.int)\n",
    "\n",
    "    ##clip at image boundaries\n",
    "    bbox[:2] = np.maximum(0, bbox[:2])\n",
    "    bbox[2:] = np.minimum(np.asarray(image.shape[:2][::-1]) - 1, bbox[2:])\n",
    "    if np.any(bbox[:2] >= bbox[2:]):\n",
    "        return None\n",
    "    sx, sy, ex, ey = bbox\n",
    "    image = image[sy:ey, sx:ex]\n",
    "    image = cv2.resize(image, tuple(patch_shape[::-1]))\n",
    "    return image\n",
    "\n",
    "\"\"\"\n",
    "    def encoder(image, boxes):\n",
    "        image_patches = []\n",
    "        for box in boxes:\n",
    "            patch = extract_image_patch(image, box, image_shape[:2])\n",
    "            if patch is None:\n",
    "                print(\"WARNING: Failed to extract image patch: %s.\" % str(box))\n",
    "                patch = np.random.uniform(\n",
    "                    0., 255., image_shape).astype(np.uint8)\n",
    "            image_patches.append(patch)\n",
    "        image_patches = np.asarray(image_patches)\n",
    "        return image_encoder(image_patches, batch_size)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "sized-lawsuit",
   "metadata": {},
   "outputs": [],
   "source": [
    "##normalisiert die bboxen f√ºr torchvision\n",
    "\n",
    "def normalize_image(image):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    image_normalized = transform(image)\n",
    "    image_normalized = image_normalized.unsqueeze(0)\n",
    "    return image_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "italian-bedroom",
   "metadata": {},
   "outputs": [],
   "source": [
    "##CNN RESNET50\n",
    "\n",
    "def ResNet50(image_normalized):\n",
    "    cnn = torchvision.models.resnet50(pretrained=True)\n",
    "    cnn = torch.nn.Sequential(*(list(cnn.children())[:-1])) \n",
    "    features = cnn(image_normalized)\n",
    "    result.view(2048)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "informal-december",
   "metadata": {},
   "outputs": [],
   "source": [
    "##command line arguments\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description=\"RESNET50 features encoder\")\n",
    "    parser.add_argument(\n",
    "        \"--mot_dir\", \n",
    "        help=\"Path to MOTChallange directory (train or test)\",\n",
    "        required=True)\n",
    "    parser.add_argument(\n",
    "        \"--output_dir\", \n",
    "        help=\"Output directory. Will be created if it does not exist.\", \n",
    "        required=True)\n",
    "    return parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "julian-queensland",
   "metadata": {},
   "outputs": [],
   "source": [
    "##main function\n",
    "\n",
    "def main():\n",
    "    args = parse_args()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
