{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "published-cream",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import\n",
    "import os\n",
    "import errno\n",
    "import argparse\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "from AP3D import API3D          ##ganzes AP3D Modul laden oder nur eine class/ ein Model aus der .py Datei? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "lesbian-geometry",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model laden und weights laden \n",
    "def load_3DCNN(model):\n",
    "    CNN3D = API3D(conv2d, time_dim=3, time_stride=1, temperature=4, contrastive_att=True)     ##API3D??? \n",
    "    \n",
    "    CNN3D.load_state_dict(torch.load('/home/ga27qef/thesis/resnet3d_mars.pth.tar'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "million-nirvana",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-4-1bfa085ab879>, line 32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-1bfa085ab879>\"\u001b[0;36m, line \u001b[0;32m32\u001b[0m\n\u001b[0;31m    \"\"\"\u001b[0m\n\u001b[0m       \n^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "#4 Bounding Box Images der selben Person   ##Imagekoordinaten aus der detection_file nehmen?\n",
    "                                           ##Wie aktualisiere ich meine 4bboxes Liste? \n",
    "                                                ## Für nächseten Zeitpunkt letzte bbox raus und neue bbox rein?\n",
    "        ##irgentwo sind schon alle bboxen von Personen\n",
    "def four_bbox_creater():\n",
    "\"\"\"\n",
    "4bboxes[]\n",
    "for 'ersten 4 Zeilen mit selber ID aus detection_file': \n",
    "    ...\n",
    "    bbox = np.array(bbox)\n",
    "    if patch_shape is not None:\n",
    "        # correct aspect ratio to patch shape\n",
    "        target_aspect = float(patch_shape[1]) / patch_shape[0]\n",
    "        new_width = target_aspect * bbox[3]\n",
    "        bbox[0] -= (new_width - bbox[2]) / 2\n",
    "        bbox[2] = new_width\n",
    "\n",
    "    # convert to top left, bottom right\n",
    "    bbox[2:] += bbox[:2]\n",
    "    bbox = bbox.astype(np.int)\n",
    "\n",
    "    # clip at image boundaries\n",
    "    bbox[:2] = np.maximum(0, bbox[:2])\n",
    "    bbox[2:] = np.minimum(np.asarray(image.shape[:2][::-1]) - 1, bbox[2:])\n",
    "    if np.any(bbox[:2] >= bbox[2:]):\n",
    "        return None\n",
    "    sx, sy, ex, ey = bbox\n",
    "    image = image[sy:ey, sx:ex]\n",
    "    image = cv2.resize(image, tuple(patch_shape[::-1]))\n",
    "    \n",
    "    image.append(4bboxes)\n",
    "    return 4bboxes\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "favorite-clerk",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-5-33582f20a98f>, line 25)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-33582f20a98f>\"\u001b[0;36m, line \u001b[0;32m25\u001b[0m\n\u001b[0;31m    '''\u001b[0m\n\u001b[0m       \n^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "#Feature Extraction\n",
    "def extract():\n",
    "'''\n",
    "def extract(model, vids, use_gpu):\n",
    "    n, c, f, h, w = vids.size()\n",
    "    assert(n == 1)\n",
    "\n",
    "    if use_gpu:\n",
    "        feat = torch.cuda.FloatTensor()\n",
    "    else:\n",
    "        feat = torch.FloatTensor()\n",
    "    for i in range(math.ceil(f/args.test_frames)):\n",
    "        clip = vids[:, :, i*args.test_frames:(i+1)*args.test_frames, :, :]\n",
    "        \n",
    "        if use_gpu:\n",
    "            clip = clip.cuda()\n",
    "        output = model(clip)\n",
    "        feat = torch.cat((feat, output), 1)\n",
    "\n",
    "    feat = feat.mean(1)\n",
    "    feat = model.bn(feat)\n",
    "    feat = feat.data.cpu()\n",
    "\n",
    "    return feat\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
